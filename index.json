[{"authors":["admin"],"categories":null,"content":"I worked as a Data Scientist intern at Amazon and Data Engineer Intern Lenovo during which I used SQL, MongoDB and Spark to wrangle with big data, and NLP, A/B testing, machine learning models and statistical methods to analyze the user behaviors.\nI am currently a Master student in Data Science at Duke University - Go Blue Devils😈! Will graduate in May 2021, I am now open to full-time Data Analyst/Data Scientist job opportunities.\nIn addition to programming and analyzing data, I enjoy oil painting and also a world explorer who has travelled 10+ countries in 3 continents\n","date":1549324800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":1555459200,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://yuan-feng1.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"I worked as a Data Scientist intern at Amazon and Data Engineer Intern Lenovo during which I used SQL, MongoDB and Spark to wrangle with big data, and NLP, A/B testing, machine learning models and statistical methods to analyze the user behaviors.\nI am currently a Master student in Data Science at Duke University - Go Blue Devils😈! Will graduate in May 2021, I am now open to full-time Data Analyst/Data Scientist job opportunities.","tags":null,"title":"Yuan Feng","type":"authors"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"8ebbbdeba622fbc24dab29b4efa6b173","permalink":"https://yuan-feng1.github.io/talk/coding/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/coding/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Coding and Analyzing Qualitative Data","type":"talk"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"a3975af5f5dadc9b2c7bbc4e48bb0e6e","permalink":"https://yuan-feng1.github.io/talk/qualitative/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/qualitative/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Qualitative Research Summer Intensive","type":"talk"},{"authors":[],"categories":null,"content":"Slides can be added in a few ways:\n Create slides using academia\u0026rsquo;s Slides feature and link using slides parameter in the front matter of the talk file Upload an existing slide deck to static/ and link using url_slides parameter in the front matter of the talk file Embed your slides (e.g. Google Slides) or presentation video on this page using shortcodes.  Further talk details can easily be added to this page using Markdown and $\\rm \\LaTeX$ math code.\n","date":1906549200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1906549200,"objectID":"f917008f74aa8012979052dcf8dbf864","permalink":"https://yuan-feng1.github.io/talk/synthesizing/","publishdate":"2017-01-01T00:00:00Z","relpermalink":"/talk/synthesizing/","section":"talk","summary":"An example talk using academia's Markdown slides feature.","tags":[],"title":"Synthesizing Qualitative Data","type":"talk"},{"authors":null,"categories":null,"content":"Summary Due to the rising concern over the pollution from the current energy sources, solar power has been regarded as one of the most promising methods to provide energy at low pollution and low environmental footprint. As of the end of 2018, solar power accounted for around 1.6% of the total power consumed in the United States amounting to 64.2 GW. Furthermore, solar power has ranked consistently as the first or second fastest growing power source in the US since 2013. Moreover, unlike nuclear, wind or hydrothermal energy solar energy can be easily harnessed from rooftops using photovoltaic cells. Solar power using photovoltaic solar panels is a quickly growing source of power across US households with increasing rates of adoption. Tracking the installation of solar panels across the nation to study the rates of adoption is often cumbersome and inaccurate - for such methods often involve self-reporting, monitoring sales of solar panels etc. A much more recent technique to track the harnessing of solar panels across the nation involves using machine learning models on satellite imagery. These models use a variety of techniques, both simple and intricate, to accurately identify the existence of solar panels in satellite images with high accuracy.\nGoals and Reports In order to obtain a thorough understanding of the current state of individual solar panel installation and usage both on a local and national scale, an automated method of identification is required for classifying the existence of solar panels based on pre-processed satellite images. In the notion of machine learning algorithms, the original model is built on the basis of an educated estimation and exploration of the dataset and trained multiple times on the training data with possible future stages of parameter tuning. The model performance will be evaluated based on its test data, which in this case are images unseen by the model beforehand. The finalized model will be able to provide classification on newly imported images in a scalable fashion. With the implementation of machine learning to solar panel image classification, the cost and time required to identify installed solar panels could be potentially reduced and will help the industry better provide renewable, clean and affordable alternative energy sources.\nIn this project, we discuss four different approaches to identify and classify solar panels in satellite images. We lay specific emphasis on two different feature extraction methods namely Histogram and Gradients (HOG) and Color Filter Feature Extraction (CFFE). We then use a K-Nearest Neighbour (KNN) Classifier in both cases to classify the images. These methods achieved an area under the curve (AUC) of 0.800 and 0.837 respectively. In our third approach, we develop an ensemble model combining both the earlier described feature extraction methods. The AUC in this case was 0.888. Finally we developed a Convolutional Neural Network (CNN) with an AUC of 0.983. We compare the different approaches and analyze where they performed well and where they can be improved.\nThe project code can be found here, and a pdf of the report is located here.\n","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"3b2ad66b611ee8c458b3089349a04425","permalink":"https://yuan-feng1.github.io/project/solar/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/project/solar/","section":"project","summary":"Summary Due to the rising concern over the pollution from the current energy sources, solar power has been regarded as one of the most promising methods to provide energy at low pollution and low environmental footprint. As of the end of 2018, solar power accounted for around 1.6% of the total power consumed in the United States amounting to 64.2 GW. Furthermore, solar power has ranked consistently as the first or second fastest growing power source in the US since 2013.","tags":["ML"],"title":"Detecting Solar Panels From Satellite Imagery","type":"project"},{"authors":null,"categories":null,"content":"Summary What to Eat is an AI-based mobile restaurant recommendation app which basically recommends restaurant by learning user preferences. The tools used for implementation were React-Native for frontend and Django for backend.\nHere are the icons for What to Eat: Dash \u0026amp; Plotly Dash is a Open Source Python library for creating reactive, Web-based applications. Dash is a user interface library for creating analytical web applications. Those who use Python for data analysis, data exploration, visualization, modelling, instrument control, and reporting will find immediate use for Dash.\nLanding Pages Upon opening the app, a short animation will pop up, allowing for selecting several cuisines of your favourotes and several other criterias. Clicing on the bubble will turn it into dark version.\nThe landing pages look like this:\nTinder-like Swiping Animation Tinder like animation was applied to the main screens when user is swiping the cards. This enabled smooth viewing and swiping experience. I researched and found shared-element-transition was supported in React Native for image transition. Location and filter functions are located on left-top and right-top of the screen, where you could re-locate your postion and pick type of cuisines and ratings for more accurate feeds. In the middle part of screen, you could switch to your like history and review your past selections\nTF-IDF Algorithms TF-IDF (term frequency–inverse document frequency) is essentially an vector written by hand, and then we can calculate the similarity between vectors. In my case, the vector is the restaurant, and the keywords are the menu items. Content based filtering was conducted by recommending restaurants similar to user favorite restaurants.\nThe raw code can be found here(where my username was listed “24451yolanda”). We are still in the process of lauching to IOS and Android strore - stay tuned!\n","date":1615852800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615852800,"objectID":"0287751af27ffd52bed84478abb6dad1","permalink":"https://yuan-feng1.github.io/project/eatnow/","publishdate":"2021-03-16T00:00:00Z","relpermalink":"/project/eatnow/","section":"project","summary":"Summary What to Eat is an AI-based mobile restaurant recommendation app which basically recommends restaurant by learning user preferences. The tools used for implementation were React-Native for frontend and Django for backend.\nHere are the icons for What to Eat: Dash \u0026amp; Plotly Dash is a Open Source Python library for creating reactive, Web-based applications. Dash is a user interface library for creating analytical web applications. Those who use Python for data analysis, data exploration, visualization, modelling, instrument control, and reporting will find immediate use for Dash.","tags":["SDE"],"title":"What to Eat - AI mobile Restaurant Recommender","type":"project"},{"authors":null,"categories":null,"content":"Project Overview The utilization of hospital resources is an urgent topic worldwide. Often we could hear in the news, especially in times of the global pandemic, of the space shortage in hospitals for patients in need. The main goal of this project is to construct a classification system built with state-of-art machine learning models to predict the discharge location of patients with ICU admission based on a series of records, such as demographics, vital signs, laboratory tests, medications information, and so on.\nThe major focus of our prediction will be on SNF which stands for skilled nursing facilities and other hospitals, Since the number of patients assigned to these facilities best represents the allocation of medical resources and hospital capacity.\nMy Responsibility  Initial operations on accessing the data \u0026amp; Data imputation on missing values. Implemented Random Forest modeling with a series of hyperparameter tuning for optimal performance. Designed workflow and layout of interactive ML real-time prediction using Flask app. Finished the real-time prediction engine using machine learning models. Data product \u0026amp; Github aggregation and calibration; Gthub readme organizations \u0026amp; updates (cooperated with all team members).  Audience Our platform aims to serve the hospital administration team, the inquisitive researchers and curious patients. The multi-page dashboard serves as a tool to monitor occupancy and clinical results of patients, and provide comprehensive demonstration. The online ML prediction model provides real-time results of clinical outcome, which will aid in decision-making process of patients and doctors.\nGoals Our project will be able to demonstrate the hospital bed occupancy and the usage of hospital resources so that the doctors are able to make better decisions. Moreover, it will display vivid visualizations of the overall trend of the patients in hospital. And our platform will help not only the doctors by the patients and inquisitive people by offering an online machine learning platform.\nData MIMIC data is a public-available database which is comprised of ~60,000 deidentified large-scale health-related data.\nData Preprocessing\nData pre-processing was conducted using multiple packages in Python, including pandas and numpy. Subsequent tasks include looking at the distribution of each varaible, converting to appropriate datatype，then checking for the missingness of the data and conducted imputation and testing performance on dummy model.\nTechnical Stack I mastered\nData processing: pandas, numpy / Visualization: pyplot, seaborn\nMachine Learning Implementation: scikit-learn, optuna, xgboost\nDeployment: dash, Flask\nGit version control: branch set-up, push, commit, merge, pull, resolve issues\nDashboard EDA First, we looked at the distribution of the MIMIC dataset. The majority of patients were discharged to their own home(58.7%), and the second-biggest location is SNF(19.9%), and the rest were sent to other facility or dead. The number of records placed on the right is a number-based illustration of this distribution. Secondly, this stacked bar chart demonstrated the admission type of patients. The majority of admitted patients were in the emergency category, newborn and elective comes second and urgent cases are the least. This indicates the classification results in the dataset was not balanced, which we will address in the modeling process for better performance. We also plotted the confusin matrix to show the correlation of this dataset. Overall, there was not much collinearity issue in this dataset.\nModel Analysis We utilized multiple machine learning algorithms to approach this classfication problem. Based on our research, the currently most popular methods include Random Forest, Xgboost, SVM and Naive Bayes. After conducting data preprocessing and dummy model as a baseline comparison, we developed the each of the models with hyper-parameter tuning for model training, and based on a series of metrics such as accuracy, precision, recall and F1, selected the optimal model for future analysis.\nrandom_grid = {\u0026#39;n_estimators\u0026#39;: n_estimators, \u0026#39;max_features\u0026#39;: max_features, \u0026#39;max_depth\u0026#39;: max_depth, \u0026#39;min_samples_split\u0026#39;: min_samples_split, \u0026#39;min_samples_leaf\u0026#39;: min_samples_leaf, \u0026#39;bootstrap\u0026#39;: bootstrap} # Use the random grid to search for best hyperparameters rf = RandomForestClassifier() # Random search of parameters, using 3 fold cross validation, search across 100 different combinations, and use all available cores rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1) # Fit the random search model rf_random.fit(X_train, y_train) Random Forest Modeling With regards to model development process, I built the Random Forest model, which is an ensemble learning method. It is trained on different parts of the data, and builds multiple decision trees during the training process then return the prediction by finding the most votes from individual trees. The major advantages of Random Forests is that it helps with our goal of boosting model performance and solving overfitting issues, but these advantages may come at the expense of some loss of interpretability.\nFor thr hyperparameter tuning process, I initially generated a grid of possible parameters including \u0026ldquo;n_estimators\u0026rdquo;, \u0026ldquo;min_samples_split\u0026rdquo;, \u0026ldquo;min_samples_leaf\u0026rdquo;, \u0026ldquo;max_features\u0026rdquo;, \u0026ldquo;max_depth\u0026rdquo;, and \u0026ldquo;bootstrap\u0026rdquo; for the model to choose from.\n#parameters of the best model rf_random.best_params_ {\u0026#39;n_estimators\u0026#39;: 1100, \u0026#39;min_samples_split\u0026#39;: 10, \u0026#39;min_samples_leaf\u0026#39;: 2, \u0026#39;max_features\u0026#39;: \u0026#39;sqrt\u0026#39;, \u0026#39;max_depth\u0026#39;: 50, \u0026#39;bootstrap\u0026#39;: False} With the help of RandomizedSearchCV function, performed gird search to find the set of parameters that returns highest accuracy values. After completing the tuning process, the best parameters are shown above.\nRandom has proved to be a very powerful model for classfication problems. The accuracy has boosted to 0.99 on train data and reached 0.68 on test data. The metrics were also satisfying for precision, recall and F1, indicating it performed well on four categories as well.\n   Table 1 - Model evaluations on train dataset       Train Dummy Classifier Logistic Regression SVM KNN Random Forest Xgboost     Accuracy 0.59 0.65 0.70 0.64 0.99 0.77   Precision 0.34 0.64 0.68 0.59 0.99 0.76   Recall 0.59 0.65 0.70 0.64 0.99 0.77   F1 0.43 0.60 0.66 0.59 0.99 0.75    The above table shows all of modeling results. All models performed quite well after cross-validation and tuning process, with an accuracy above 64% for all models except dummy model. Random Forest ranks top in terms of performance on train data. It has almost perfect results in terms of classification.\n   Table 2 - Model evaluations on test dataset       Test Dummy Classifier Logistic Regression SVM KNN Random Forest Xgboost     Accuracy 0.59 0.65 0.60 0.66 0.68 0.68   Precision 0.34 0.60 0.63 0.55 0.65 0.65   Recall 0.59 0.65 0.66 0.60 0.68 0.68   F1 0.43 0.59 0.62 0.53 0.64 0.66    However, when we look at results on test data, random forest model performed roughly the same as Xgboost modeling. The accuracy of Random Forest is 0.68, which is much lower than on train data, which indictas potential issues of overfitting. Based on a combination of performance and consistency, we chose Xgboost which has the highest accuracy of 68% with a overall satisfactory performance on all four categories. All F1, recall and precision values are aligned.\nReason why Xgboost outperformed Our results are aligned in practice of various data science competitions and it also makes sense because for a theoretical standpoint. Xgboost is essentially a very powerful model especially for classification problems. XGBoost stands for extreme gradient boosting. Boosting is an ensemble technique which trains models in sequence rather than training models separately. Each new model will correct the errors made by the previous ones and are added sequentially until no further improvements are available.\nOnline ML Real-Time Prediction Online real-time prediction is one of the unique features of our data product. I implemented this platform, from designing the initial layout, seraching for best tools to implementation. The backend I decided to use is Flask, because it is lightweight with simple syntax comparing with other backend options, and it is based on Python which has higher popularity in DS field. Moreover, Flask app could be combined with Dash to create fascinating visualization effects, and it integrates perfectly with the dashboard.\nTotal of 10 variables are available for users to manually input with descriptions by the side. Five authors features are the top five are featuring protest plot while the rest are features with higher availability to the general public. There are over 80 features in our data and they will be unrealistic for user input every variable. Below is the input section:\nThis was a great opportunity for me to brush-up on web dev skills. I used HTML and CSS for writing the structure and decoration of this interface, and callbacks in Javascript to render the inputs. Here shows a part of code to demonstarte this process.\nserver = flask.Flask(__name__) app = dash.Dash(__name__, server=server,external_stylesheets=external_stylesheets) layout = html.Div([ dbc.Container([ dbc.Row([ dbc.Col(html.H1(\u0026#34;Real Time Prediction Serverless App\u0026#34;), className=\u0026#34;mb-2\u0026#34;) ]), dbc.Container([ dbc.Row([dbc.Col(dbc.Card(html.H3(children=\u0026#39;Input Features\u0026#39;,className=\u0026#34;text-center text-light bg-dark\u0026#34;), body=True, color=\u0026#34;dark\u0026#34;), className=\u0026#34;mt-4 mb-4\u0026#34;)]) ]), dbc.FormGroup([dbc.Label([\u0026#34;Binary Input: if insurance type is \u0026#39;medicare\u0026#39; then 1 else 0\u0026#34;]), dbc.Input(id=\u0026#34;INSURANCE_Medicare\u0026#34;,type=\u0026#34;number\u0026#34;, placeholder=\u0026#34;Enter or select...\u0026#34;,min=0, max=1),]), @app.callback( Output(\u0026#34;out\u0026#34;, \u0026#34;children\u0026#34;), [Input(\u0026#34;show\u0026#34;, \u0026#34;n_clicks\u0026#34;)], state=[State(\u0026#34;Age\u0026#34;, \u0026#34;value\u0026#34;), State(\u0026#34;Gender\u0026#34;, \u0026#34;value\u0026#34;), State(\u0026#34;HeartRate_Mean\u0026#34;, \u0026#34;value\u0026#34;), State(\u0026#34;Glucose_Mean\u0026#34;, \u0026#34;value\u0026#34;), Therefore, besides these 10 selected features, I used mode or mean of the rest remaining variables available to public such as age, sex, and heart rate level, etc.There are over 80 features in our data and it will be unrealistic for user to input every variable. Therefore besides these 10 selected features, we use mod or mean of the rest remaining variables.\nThere are over 80 features in our data and they will be unrealistic for user input every variable. Therefore besides these 10 selected features, we use mode or meaning of the remaining variables.\nOnce the user finished all the required input. Upon clicking the go button our cloud-based model will display the most accurate prediction based on the input. Here we can see that the best gas according to my input is home which means the patient with such features is most likely to be discharged to home.\nMy takeaways from this project I\u0026rsquo;m really glad to have this great opportunity to work with MIMIC EHR dataset, I have always been intrigued by application of Data Science in biomedical filed and this final project marked a milestone on my trajectory into personal \u0026ldquo;wild west\u0026rdquo;.\nThis was a good combination of skills learned in class and real-world challenges, and I feel very satisfied to successfully implement a complete inteface based on ML modeling and analysis. Big shout-out to my teammates for a semester\u0026rsquo;s hard work.😊 Thanks to Prof. Chan and TA for all the precious instructions \u0026amp; suggestions.🎉\nI hope you\u0026rsquo;re enjoying this blog article and find our data product interesting. Here is the Interactive App. GitHub Link of models is here. GitHub Link of dashboard can be found here.\nThanks for reading！\nReferences Plotly website https://plotly.com/\n","date":1615766400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615766400,"objectID":"9761ed822123e4df8b1d4a1b17762465","permalink":"https://yuan-feng1.github.io/project/hospital/","publishdate":"2021-03-15T00:00:00Z","relpermalink":"/project/hospital/","section":"project","summary":"Project Overview The utilization of hospital resources is an urgent topic worldwide. Often we could hear in the news, especially in times of the global pandemic, of the space shortage in hospitals for patients in need. The main goal of this project is to construct a classification system built with state-of-art machine learning models to predict the discharge location of patients with ICU admission based on a series of records, such as demographics, vital signs, laboratory tests, medications information, and so on.","tags":["ML"],"title":"Patient Discharge Location Predicion with MIMIC Dataset","type":"project"},{"authors":null,"categories":null,"content":"PHD in US This dataset comes from PhDs awarded in the US. It collects data on the number and characteristics of individuals receiving research doctoral degrees from U.S. academic institutions. Based on its official website, this survey included surveys from individuals receiving a research doctorate from a U.S. academic institutions in various fields.\nDash \u0026amp; Plotly Dash is a Open Source Python library for creating reactive, Web-based applications. Dash is a user interface library for creating analytical web applications. Those who use Python for data analysis, data exploration, visualization, modelling, instrument control, and reporting will find immediate use for Dash.\nAnalysis Online dashboard can be found here.\nThere are three plots included in this Dashboard. I included screenshots in this post. For the interactive version. Dashboard here](http://noafeng72.pythonanywhere.com).\nThis table illustrates the median salary of PHDs in 2017. As we could see, there is a quite large between different fields. The salaries in Math \u0026amp; CS are higher than the rest of areas overall sectors, and PHDs who work in the industry have higher salaries than rest of sectors overall.\nThis table illustrates the debt situation of PHDs from 2008 to 2017. The majority of PHDs have no debts, while there are significant number of PHDs who have quite high debts (over 30,000 USD) in their course of PHD study. Based on this graph, the financial situation of PHDs is overall stable.\nThis table illustrates the number of PHDs recipients from 2008 to 2017. We can see from the plot for each field, the number of PHDs recipients are increasing. To be more specific, the area of life sciences has the biggest increase, this is a sign of scientific focus of the US society in life science areas such as healthcare. Overall, the number of PHDs recipients and this trend is seemingly to maintain, I can estimate that there will be more PHDs in the following years.\nDash Code Overall, the code style in Dash is similar to other plotting packages. There are plenty of tutorials on plotly official website for Python implementation of Dash. Following is a part of my full code, that calls the part of figure and place them onto the the dashboard.\napp.layout = html.Div([ html.H1(children=\u0026#39;Dashboard on Science \u0026amp; Engineering Doctorates\u0026#39;), dcc.Graph(id=\u0026#34;fig22\u0026#34;), dcc.Graph( id=\u0026#39;bar1\u0026#39; ), dcc.Slider( id=\u0026#39;year1\u0026#39;, min=2008, max=2017, value=2008, marks={str(year): str(year) for year in year_unique}, step=None ), dcc.Graph( id=\u0026#39;Number of Doctorate recipients 1987 - 2007\u0026#39;, figure=fig11 ) Deployment One easy approach is to use:pythonanywhere. I followed this introduction on Medium.\nFirst, we need to sign up to pythonanywhere.com by creating a Beginner account. On the top bar go to Web \u0026gt; Add a new web app, the select Flask as the Python Web framework. You need to choose the version of Python that works best for your peoject.\nThe next step is uploading the python file. On the top bar go to Files and, in the Directories sidebar, click on mysite/. You\u0026rsquo;ll find a file named flask_app.py inside. You can delete the default empty python file.\nThe third step is to install the dependencies. On the Consoles tap, we’ll find the Bash console, this is the same as well as the Python console. Then we can start uploading the files we generated.\nLast step is to change the code in Web and in the Code section after opening the WSGI configuration file based on your code and board implementation.\n","date":1615680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615680000,"objectID":"9723547690e3a2bfcc5abe05ad0b6994","permalink":"https://yuan-feng1.github.io/project/phd/","publishdate":"2021-03-14T00:00:00Z","relpermalink":"/project/phd/","section":"project","summary":"PHD in US This dataset comes from PhDs awarded in the US. It collects data on the number and characteristics of individuals receiving research doctoral degrees from U.S. academic institutions. Based on its official website, this survey included surveys from individuals receiving a research doctorate from a U.S. academic institutions in various fields.\nDash \u0026amp; Plotly Dash is a Open Source Python library for creating reactive, Web-based applications. Dash is a user interface library for creating analytical web applications.","tags":["DV"],"title":"Dahsboard on US PHD Recipients","type":"project"},{"authors":null,"categories":null,"content":"Introduction to CNN A convolutional neural network (CNN) is a specific type of artificial neural network that uses perceptrons, a machine learning unit algorithm, for supervised learning, to analyze data. CNNs apply to image processing, natural language processing and other kinds of cognitive tasks. They are also the go-to deep learning architecture for computer vision tasks, such as object detection, image segmentation, facial recognition, etc.\nImage Classification Image classification is one of popular use-case for CNN. In this hands-on tutorial, we will leverage Keras, a python based deep learning framework to build the CNN model to classify the type of insects from insect images.\nClassification is the process of categorizing images based on their features. Theses feature are usually represented by the significant edges in an image, the level of pixel density, the different pixel values, etc. With regard to the insect dataset, the shapes and edges of insects could be different, the colors in pixels values may also vary, and these indicators may help indentify the specific insect images, and we hope to find thses specific pattern across these images by converting them into matrices of pixel values and further feed these data into deep learning models.\nOne example of this preprocessing is shown below. After converting the images into matrices of pixels, we could construct the original image with the following code, which will return the first image in train dataset.\nimg = plt.imshow(X_train[0][:,:,::-1])  Figure 1 - Image of Dragonfly  Components in CNN The workflow is as follows: For each input image in CNN models, it will pass it through a series of convolution layers with filters (Kernals), then the Pooling process, then will pass through fully connected layers (FC) and lastly, apply Softmax function to classify an object, and return the probabilistic values between 0 and 1.\nConvolution\nA convolution is a mathematical operation applied on a matrix. This matrix is usually the image represented in the form of pixels/numbers. The convolution operation extracts the features from the image.  Figure 2 - Convolution Example  Padding\nPadding is simply a process of adding layers of zeros to our input images so as to avoid the problems mentioned above. This prevents shrinking as, if p = number of layers of zeros added to the border of the image, then our (n x n) image becomes (n + 2p) x (n + 2p) image after padding\nReLu layer\nThe importance of ReLU is to introduce non-linearity in our CNN model. This is an element-wise operation (applied per pixel) which will replaces all negative pixel values by 0 in the feature map. Convolution is a linear operation – element wise matrix multiplication and addition, so we account for non-linearity by introducing a non-linear function like ReLU.\nPooling\nThe reason why we introduce Spatial Pooling (also called subsampling or downsampling) is that it will reduce the dimensionality of each feature map but retains the most important information. Spatial Pooling can be of different types: Max, Average, Sum etc. In case of Max Pooling, we define a spatial neighborhood (for example, a 2×2 window) and take the largest element from the rectified feature map within that window. In cases of Average Pooling, we could take the average values, and in case of sum pooling, we will use the sum of all elements in that window. In practice, Max Pooling has been shown to perform the best.  Figure 2 - Pooling Example  Fully connected layer\nThen we will flatten the output of the last ReLu layer. (Flattening means we convert it to a vector.) The vector values are then connected to all neurons in the fully connected layer. When the model is able to detect higher level features in the input images, it can then function as an input for a fully connected layer.  Figure 2 - Fully connected layer Example  Code Implementation Below is the Python keras inplementation of using the insects dataset to classify the type of insects. After pre-processing mentioned previously, we then construct the CNN model and fit the train dataset into the model. Here I use 50 epochs and batch size of 64, and as shown in the screenshot, the accuracy kept increasing as the training proceed.\nmodel = keras.models.Sequential() model.add(Conv2D(32, (5, 5), activation=\u0026#39;relu\u0026#39;, input_shape=(84,84,3))) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Conv2D(32, (5, 5), activation=\u0026#39;relu\u0026#39;)) model.add(MaxPooling2D(pool_size=(2, 2))) model.add(Flatten()) model.add(Dense(1000, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.5)) model.add(Dense(500, activation=\u0026#39;relu\u0026#39;)) model.add(Dropout(0.5)) model.add(Dense(250, activation=\u0026#39;relu\u0026#39;)) model.add(Dense(10, activation=\u0026#39;softmax\u0026#39;)) Then after the training is completed, the model will be compiled and fit on train dataset. The accuracy on test data is 0.73 and loss is 1.60. These are not optimal as of now, there may be overfitting issues, which could be fixed by further tuning the model and using cross-validation techniques in the future.\nmodel.compile( loss=\u0026#39;categorical_crossentropy\u0026#39;, metrics=[\u0026#39;accuracy\u0026#39;]) model.fit(X_train, y_train, epochs=50, batch_size=64) test_loss, test_acc = model.evaluate(X_test, y_test) Sources Convolutional Neural Networks (CNNs / ConvNets)\nAn Intuitive Explanation of Convolutional Neural Networks\n","date":1615680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615680000,"objectID":"8bb7d52e8a79dcde00315ec7a6c6775f","permalink":"https://yuan-feng1.github.io/project/insect/","publishdate":"2021-03-14T00:00:00Z","relpermalink":"/project/insect/","section":"project","summary":"Introduction to CNN A convolutional neural network (CNN) is a specific type of artificial neural network that uses perceptrons, a machine learning unit algorithm, for supervised learning, to analyze data. CNNs apply to image processing, natural language processing and other kinds of cognitive tasks. They are also the go-to deep learning architecture for computer vision tasks, such as object detection, image segmentation, facial recognition, etc.\nImage Classification Image classification is one of popular use-case for CNN.","tags":["ML"],"title":"Deep Learning - Insects Classification","type":"project"},{"authors":null,"categories":null,"content":"Star Wars API \u0026amp; Request Requests is one of the most popular libraries that helps retrieve data from the Internet. I will utilize the Star Wars API, which includes information about all characters from Stsr Wars, and Request to collect online data in Python and perform data wrangling and analysis.\nLoading Data with Request   First, we take a look at Request and download the data of the first person in the database. By giving the link of first person and convert the returned values to json format, we successfully retrieved information about Luke Skywalker.\nimport requests one = requests.get(\u0026#39;https://swapi.dev/api/people/1\u0026#39;) char = one.json() char Notice that the API only returns the information in the current page passed in the request function. In order to retrieve all characters from database, a for loop that iterates over all non-empty pages will be applied.\n    Getting All Characters from Star Wars   The following code shows how to download the information about each person and then transform into a complete list.\nBy initiating an empty list and a pending the results of each request to that list, You\u0026rsquo;ll be able to get an entire list of all people appeared in the Star Wars API.\ninstance = requests.get(\u0026#34;https://swapi.dev/api/people/\u0026#34;) all = list() while instance.json()[\u0026#34;next\u0026#34;]: all += instance.json()[\u0026#39;results\u0026#39;] instance = requests.get(instance.json()[\u0026#39;next\u0026#39;]) all += instance.json()[\u0026#39;results\u0026#39;] In order to better understand the retrieved information, I transformed the J some format into a panda dataframe using the \u0026ldquo;json_normalize\u0026rdquo; function. Below a preview of what the date it looks like.\nimport pandas as pd df = pd.json_normalize(all) df.head() Query to Find the Oldest   Perfect! Now that we have the data set, we can go ahead and start some interesting analysis. Here we hope to find the name of the oldest person.\nIn the world of Star Wars, the way they record the time is by ABY or BBY, which means “After Battle of Yavin” / “Before Battle of Yavin”. We do not need to worry about this, because all people in our dataset are classified as BBY.\nbirth = df[\u0026#34;birth_year\u0026#34;] import numpy as np birth = birth.str.replace(\u0026#39;BBY\u0026#39;,\u0026#39;\u0026#39;).replace(\u0026#39;unknown\u0026#39;,0).astype(\u0026#39;float\u0026#39;) df[\u0026#34;age\u0026#34;] = birth df.sort_values(by=[\u0026#39;age\u0026#39;], ascending=False).head(1) Then we could do a simple data cleaning by removing the BBY in the \u0026ldquo;age\u0026rdquo; columnm, transform string formatted numbers into double, and replace missing values with 0.Lastly, by performing a sort on the age column in a descending fashion, you\u0026rsquo;ll get the line of the olderst person - Yoda! that explains why he is so wise.\nWhat Movies Does He Appear in?   Now that we have Yoda, As somebody who has never watch Star Wars movies before, I am really interested in finding out what movies he appeared in. However, our current dataset only shows a list of available urls to retrieve the films information. So we need to construct a for loop, and then use request to get the title from the information retrieved using these URLs.\nfilms = [x for x in df.sort_values(by=[\u0026#39;age\u0026#39;], ascending=False).head(1)[\u0026#34;films\u0026#34;]][0] film_names = [requests.get(i).json()[\u0026#39;title\u0026#39;] for i in films] film_names I also concatenated all the responses into a list. We can see that Yoda appeared in The Empire Strikes Back, Return of the Jedi, The Phantom Menace, Attack of the Clones, and Revenge of the Sith.\nIf you want to find out more about the process in detail. Here is detailed process ans source code on my Github.\n","date":1615680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615680000,"objectID":"ee207eb3d1baf034e521755fc4de7a8c","permalink":"https://yuan-feng1.github.io/project/starwars/","publishdate":"2021-03-14T00:00:00Z","relpermalink":"/project/starwars/","section":"project","summary":"Star Wars API \u0026amp; Request Requests is one of the most popular libraries that helps retrieve data from the Internet. I will utilize the Star Wars API, which includes information about all characters from Stsr Wars, and Request to collect online data in Python and perform data wrangling and analysis.\nLoading Data with Request   First, we take a look at Request and download the data of the first person in the database.","tags":["ML"],"title":"Star Wars Data with Request","type":"project"},{"authors":null,"categories":null,"content":"Summary This tutorial explains the concepts and motivations behind transfer learning and seeks to provide an example of the technique used in practice based on my unerstanding and experience.\nIn the supervised learning context, deep neural networks have become more and more accurate in recent years. Convolutional neural networks using a residual learning network are particularly good at finding the optimal model parameters while improving accuracy. However, it is not possible to apply these deep models into a different situation or phenomena other than what they were trained and designed for. It is important to have this flexibility in real-world scenarios where we rarely encounter data structured in the same way as our training data. Transfer learning allows us to transfer some of the information learned from an ideal situation into a different and related situation for which our data is more limited.\nAbove are sample images from the hymenoptera_data dataset. In this article we will define a common transfer learning technique for computer vision and image classification problem: inductive transfer learning using feature extraction or embedding using the hymenoptera_data dataset. By using a model previously trained on a robust training dataset, we can take advantage of its optimal parameters as a way to extract features for new data, reducing training time and improving performance considerably. First, we shall implement this method classically. Later, we will discuss one particular cutting-edge approach to this problem using quantum computing for a hybrid transfer learning method. In this approach, we replace a neural network’s output layer with a dressed quantum circuit that we then train for our target domain and task.\nResults Here is the model performance:\nTransfer learning techniques can improve significantly on similar models built from scratch. Training time can be reduced by one half with classical parameter transfer methods, and performance in terms of AUC, AP and accuracy can also be improved significantly. These results can be attained with a minimum amount of training data as long as there is a pre-existing model trained in a similar domain. Both the classical and the CQ hybrid approach accomplished very similar performance in terms of accuracy and AUC. This both highlights the advantages of transfer learning and positive outlook of novel quantum methods to perform classifications.\nThe project report can be found here.\n","date":1615680000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615680000,"objectID":"1909d2c2b5d64d399612b0e4523a29fa","permalink":"https://yuan-feng1.github.io/project/tranfer/","publishdate":"2021-03-14T00:00:00Z","relpermalink":"/project/tranfer/","section":"project","summary":"Summary This tutorial explains the concepts and motivations behind transfer learning and seeks to provide an example of the technique used in practice based on my unerstanding and experience.\nIn the supervised learning context, deep neural networks have become more and more accurate in recent years. Convolutional neural networks using a residual learning network are particularly good at finding the optimal model parameters while improving accuracy. However, it is not possible to apply these deep models into a different situation or phenomena other than what they were trained and designed for.","tags":["ML"],"title":"Transfer Learning Tutorial","type":"project"},{"authors":null,"categories":null,"content":"Spotify Songs Data Understanding and managing data is one of the priorities of analysts and data scientists. Relational databases are common and useful tools for performing tasks in selected database. Both querying and building databases are essential functionaries of relational databases. This post will focus on constructing databases in SQL with data from Spotify songs. Various information including track name, album name and track characteristics are provided in this dataset. Analysis will be also performed on the intrumentalness of songs in playlists.\nDatabase Basics In order for databased to run efficiently, normalization is a common technique to reduce redundancy in storaging the information. Normalization derives the relationships of contents and stores the dataset in separate tables.\n First Normal Form (1NF)  This is the basic requirement for database efficiency where we require four must-have features: Each column name is unique; Atomic values in each cell; The values in one column should belong to one type and the order does not matter.\n Second Normal Form (2NF)  We then come to Second Normal Form where there are more requirements based on 1NF: Now no partial dependency is allowed. It means that there cannot be dependencies other than the Primary Key, which is a set of values that could uniquely identify a record. And we must find each value based on the complete Primary Key.\n Third Normal Form (3NF)  In 3NF, besides 1NF and 2NF requirements, we are not allowed to have transitive dependencies: Non-Primary Key columns in the data cannot contain information about the rest of columns.\nOur Schema If we know the track ID, we could find its name, release date and artist. And based on the track name, we will then find about album name, album release date, etc. Therefore, the best schema is to separate them into different tables to achieve Third Normal Form:\n   Table Columns Included     track_id_playlist_id Track ID and Playlist ID   track_id_album_id Track ID and Album ID   playlist_id_album_id Playlist ID and Album ID   album_id_album_name Album ID and Album Name   album_id_album_date Album ID and Release Date   playlist_id_name Playlist ID and Name   playlist_id_genre Playlist ID and Genre   playlist_id_subgenre Playlist ID and Subgenre   track_id_track_name Track ID and Track names   track_id_track_artist Track ID and artist names   track_id_more_details Track ID and other features    Database Constrcution With the help of SQLite, database construction is now an easy process, we can connect to the server using just a few lines of code and upload dataframes directly into the created database:\n%load_ext sql %sql sqlite:// %sql PERSIST track_id_album_id; %sql PERSIST track_id_playlist_id; ... Here is detailed process ans source code on my Github.\nDatabase Test Run We can test the functionality of the database by running some queries to solve a question of our interest. Here we want to see find the names of all playlists that contain instrumentals.\nBased on the data dictionary, we know songs with \u0026ldquo;instrumentals\u0026rdquo; bigger than .5 are qualified for this question. To find the names of such playlists, we need information from \u0026ldquo;track_id_playlist_id\u0026rdquo;, \u0026ldquo;track_id_more_details\u0026rdquo; and \u0026ldquo;playlist_id_name\u0026rdquo;. The following code first selects playlist ID that has instrumental songs and saves as temporary table, then join the \u0026ldquo;playlist_name\u0026rdquo; table to find the names corresponding with these track IDs.\n%%sql create table temp as select distinct(p.playlist_id) from track_id_playlist_id as p left join track_id_more_details as t on t.track_id = p.track_id where t.instrumentalness \u0026gt; 0.5; select distinct(p.playlist_name) from temp left join playlist_id_name as p on p.playlist_id = temp.playlist_id DROP TABLE IF EXISTS temp; Now our database is successfully implemented! As we could see below, the code will return a list of all playlists that contain instrumentals, which means it\u0026rsquo;s running as expected:\n","date":1615593600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615593600,"objectID":"7649dfc75d7c0e142575611058310187","permalink":"https://yuan-feng1.github.io/project/spotify/","publishdate":"2021-03-13T00:00:00Z","relpermalink":"/project/spotify/","section":"project","summary":"Spotify Songs Data Understanding and managing data is one of the priorities of analysts and data scientists. Relational databases are common and useful tools for performing tasks in selected database. Both querying and building databases are essential functionaries of relational databases. This post will focus on constructing databases in SQL with data from Spotify songs. Various information including track name, album name and track characteristics are provided in this dataset. Analysis will be also performed on the intrumentalness of songs in playlists.","tags":["ML"],"title":"Spotify Song Database","type":"project"},{"authors":null,"categories":null,"content":"Malaria is caused by parasites from the genus Plasmodium, which could be spread to people by the bite of infected mosquitoes. In 2017, there were 219 million malaria cases that led to 435,000 deaths Malaria is an urgent public health priority for past decade. It has been estimated that around half of the world population are at risk today. Therefore, analysis on Malaria is useful in terms of global public health for research in preventive measures. I would like to share three visualizations using an online dataset. These plots are generated by Python. Link to the Jupyter Notebook\n Plot 1 Avg. Malaria Deaths per 100,000 people (1990-2016)  Based on recent statistics, Africa carries a disproportionately high portion of the global malaria burden. As shown from the following heatmap measuring the number of death per 1000 people worldwide from 1990 to 2016. By the heavy concentration of red color on the map, it seems that most malaria cases and deaths occur in sub-Saharan Africa.\n Plot 2 Number of Worldwide Malaria Deaths categorized by Age (1990-2016)  Most victims of Malaria are children. It is estimated to be one of the leading causes of child mortality. This could be demonstrated by the following plot. From 1990-2016, in terms of the total Malaria deaths, 77% of them are children under 5.\n Plot 3 Number of Top 10 Countries with most Malaria Deaths (1990-2016)  Noy only sub-Saharan Africa has suffered most from Malaria, Nigeria and Congo are countries most affected by Malaria in the world. Nigeria alone accounts for 29.1% of total deaths caused by Malaria, and this number at Congo is 11.3%. The 10 most affected countries took up 76% of total Malaria deaths while the rest of world only accounts for 24.0% (1990-2016). Medical prevention of Malaria should be regarded as a top priority for these countries.\nThat\u0026rsquo;s all for this blog. Thanks for reading!\nLet me know if you have any comments/question. Feel free to find me at yuan.feng347@duke.edu\n","date":1615507200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615507200,"objectID":"335087c38356af98c62348445b9b5146","permalink":"https://yuan-feng1.github.io/project/malaria/","publishdate":"2021-03-12T00:00:00Z","relpermalink":"/project/malaria/","section":"project","summary":"Malaria is caused by parasites from the genus Plasmodium, which could be spread to people by the bite of infected mosquitoes. In 2017, there were 219 million malaria cases that led to 435,000 deaths Malaria is an urgent public health priority for past decade. It has been estimated that around half of the world population are at risk today. Therefore, analysis on Malaria is useful in terms of global public health for research in preventive measures.","tags":["DV"],"title":"Visualizing Malaria Death","type":"project"},{"authors":null,"categories":null,"content":"A portfolio website is a great choice for you to showcase your work and let recruiters know more about you. With Github Page, we could easily set up a personal website for free. And using Hugo, you can have access to hundreds of special themes to choose from.\nFor me, I\u0026rsquo;m using Mac OS system and plan to build up this website from command line. Below is a detailed tutorial of how I successfully set up my website from scratch - Stay Tuned!\nLink to my Github repository\n Install Hugo and Git  First open the terminal, and input the following command-line. Note that if you do not have homebrew installed, you may need to install it first.\nbrew install hugo I recommend installing Git as well. run git --version in your command-line for checking.\n Build on local machine  Make sure to sit inside the directory to save the website. And run the following to initialize a new site:\ncd /Users/yuanfeng/Desktop hugo new site site_name Then, we should have a new folder with the name we just made. It will come with a list of folders, and we will dig deeper into those folders in a minute.\n Select a theme  I personally enjoy designs of minimalist and the use of white or black as its main colors. It only takes googling to find ready-to-go themes but it took me a while to find my favorite.\nThen I run the following command line in the terminal to download the contents in themes folder:\ncd site_name cd themes git clone https://github.com/themefisher/academia-hugo And it\u0026rsquo;s also necessary to make sure the configurations are correct. Here I made some changes in the config.toml.\nbaseurl = \u0026#34;https://[your github username].github.io/\u0026#34; languageCode = \u0026#34;en-us\u0026#34; title = \u0026#34;the website title shown on the tab\u0026#34; theme = \u0026#34;the same as theme name (exactly the folder name) in your themes folder\u0026#34;  Test locally  I wanted to see how the webpage currently looks like by running the following:\ncd site_name hugo server The following indicated success in running it locally. By going to http://localhost:1313/ in my web browser, I could see my work so far and observe changes real-time.\nBuilt in 4 ms Watching for changes in /Users/yuanfeng/site/yuan/{archetypes,content,data,layouts,static} Watching for config changes in /Users/yuanfeng/site/yuan/config.toml Environment: \u0026#34;development\u0026#34; Serving pages from memory Running in Fast Render Mode. For full rebuilds on change: hugo server --disableFastRender Web Server is available at http://localhost:1313/ (bind address 127.0.0.1) Press Ctrl+C to stop  Personalize my website  The template comes with a variety of different sections such as Publications, Recent Posts and Contact. However, in order for the main page to stay concise and minimal, I removed the less appealing and related components and replaced the sample photos and texts with mine by editing html and md files.\nAnother main change compared with the original template is the header and dropdown. This turned out to be tricky as I needed to tweak the Javascript code as well. The final results seem great!\n Deploy to Git Pages  Last step is to render my website to Github Pages. I started by running the following commnad, which will generate a folder called public and it will contain the required web files.\ncd site_name hugo  Create Github Repository  I wanted to publish my site as my github name. In order to do that, I created a new branch named \u0026lt;USERNAME\u0026gt;.github.io. and set it to public.\nThen the following command could sync the website to my github.\ncd public git init git add . git remote add origin https://github.com/username/username.github.io.git git commit -m “first commit” git push origin master After completing the committing, I went to my repository and clicked on settings, then scrolled to the section of Github Pages. It successfully returned the link of my personal website:\nhttps://yuan-feng1.github.io/\nAnd my personal website was successfully published.\nThat\u0026rsquo;s all for this blog. Thanks for reading!\nLet me know if you have any comments/question. Feel free to find me at yuan.feng347@duke.edu\n","date":1615334400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615334400,"objectID":"1c2a5dd8f04c087f5316de50fdf179f0","permalink":"https://yuan-feng1.github.io/project/website/","publishdate":"2021-03-10T00:00:00Z","relpermalink":"/project/website/","section":"project","summary":"A portfolio website is a great choice for you to showcase your work and let recruiters know more about you. With Github Page, we could easily set up a personal website for free. And using Hugo, you can have access to hundreds of special themes to choose from.\nFor me, I\u0026rsquo;m using Mac OS system and plan to build up this website from command line. Below is a detailed tutorial of how I successfully set up my website from scratch - Stay Tuned!","tags":["Demo"],"title":"How I built my own portfolio website","type":"project"},{"authors":null,"categories":null,"content":" Problem 13 Large sum - Solved by 224972  In this question, we are asked to:\nWork out the first ten digits of the sum of the following one-hundred 50-digit numbers. link here\nThe thought process of this problem is quite straightforward: we first sum up the one-hundred 50-digit numbers, then with the result, turn it into a string, then slicing the first ten digits would solve the problem.\nIn python, this is actually an easy process, using the built-in sum() and str() function, we could simply generate the sum, then turn it into a string and slice the first 10 digits.\ndef Solution_13(nums): \u0026#34;\u0026#34;\u0026#34; The sum of nums array, transform into string, take the first 10 digits \u0026#34;\u0026#34;\u0026#34; ttl = sum(nums) answer = str(ttl)[:10] return answer nums = [ 37107287533902102798797998220837590246510135740250, 46376937677490009712648124896970078050417018260538, 74324986199524741059474233309513058123726617309629, 91942213363574161572522430563301811072406154908250, 23067588207539346171171980310421047513778063246676, 89261670696623633820136378418383684178734361726757, 28112879812849979408065481931592621691275889832738, 44274228917432520321923589422876796487670272189318, 47451445736001306439091167216856844588711603153276, 70386486105843025439939619828917593665686757934951, 62176457141856560629502157223196586755079324193331, 64906352462741904929101432445813822663347944758178, 92575867718337217661963751590579239728245598838407, 58203565325359399008402633568948830189458628227828, 80181199384826282014278194139940567587151170094390, 35398664372827112653829987240784473053190104293586, 86515506006295864861532075273371959191420517255829, 71693888707715466499115593487603532921714970056938, 54370070576826684624621495650076471787294438377604, 53282654108756828443191190634694037855217779295145, 36123272525000296071075082563815656710885258350721, 45876576172410976447339110607218265236877223636045, 17423706905851860660448207621209813287860733969412, 81142660418086830619328460811191061556940512689692, 51934325451728388641918047049293215058642563049483, 62467221648435076201727918039944693004732956340691, 15732444386908125794514089057706229429197107928209, 55037687525678773091862540744969844508330393682126, 18336384825330154686196124348767681297534375946515, 80386287592878490201521685554828717201219257766954, 78182833757993103614740356856449095527097864797581, 16726320100436897842553539920931837441497806860984, 48403098129077791799088218795327364475675590848030, 87086987551392711854517078544161852424320693150332, 59959406895756536782107074926966537676326235447210, 69793950679652694742597709739166693763042633987085, 41052684708299085211399427365734116182760315001271, 65378607361501080857009149939512557028198746004375, 35829035317434717326932123578154982629742552737307, 94953759765105305946966067683156574377167401875275, 88902802571733229619176668713819931811048770190271, 25267680276078003013678680992525463401061632866526, 36270218540497705585629946580636237993140746255962, 24074486908231174977792365466257246923322810917141, 91430288197103288597806669760892938638285025333403, 34413065578016127815921815005561868836468420090470, 23053081172816430487623791969842487255036638784583, 11487696932154902810424020138335124462181441773470, 63783299490636259666498587618221225225512486764533, 67720186971698544312419572409913959008952310058822, 95548255300263520781532296796249481641953868218774, 76085327132285723110424803456124867697064507995236, 37774242535411291684276865538926205024910326572967, 23701913275725675285653248258265463092207058596522, 29798860272258331913126375147341994889534765745501, 18495701454879288984856827726077713721403798879715, 38298203783031473527721580348144513491373226651381, 34829543829199918180278916522431027392251122869539, 40957953066405232632538044100059654939159879593635, 29746152185502371307642255121183693803580388584903, 41698116222072977186158236678424689157993532961922, 62467957194401269043877107275048102390895523597457, 23189706772547915061505504953922979530901129967519, 86188088225875314529584099251203829009407770775672, 11306739708304724483816533873502340845647058077308, 82959174767140363198008187129011875491310547126581, 97623331044818386269515456334926366572897563400500, 42846280183517070527831839425882145521227251250327, 55121603546981200581762165212827652751691296897789, 32238195734329339946437501907836945765883352399886, 75506164965184775180738168837861091527357929701337, 62177842752192623401942399639168044983993173312731, 32924185707147349566916674687634660915035914677504, 99518671430235219628894890102423325116913619626622, 73267460800591547471830798392868535206946944540724, 76841822524674417161514036427982273348055556214818, 97142617910342598647204516893989422179826088076852, 87783646182799346313767754307809363333018982642090, 10848802521674670883215120185883543223812876952786, 71329612474782464538636993009049310363619763878039, 62184073572399794223406235393808339651327408011116, 66627891981488087797941876876144230030984490851411, 60661826293682836764744779239180335110989069790714, 85786944089552990653640447425576083659976645795096, 66024396409905389607120198219976047599490197230297, 64913982680032973156037120041377903785566085089252, 16730939319872750275468906903707539413042652315011, 94809377245048795150954100921645863754710598436791, 78639167021187492431995700641917969777599028300699, 15368713711936614952811305876380278410754449733078, 40789923115535562561142322423255033685442488917353, 44889911501440648020369068063960672322193204149535, 41503128880339536053299340368006977710650566631954, 81234880673210146739058568557934581403627822703280, 82616570773948327592232845941706525094512325230608, 22918802058777319719839450180888072429661980811197, 77158542502016545090413245809786882778948721859617, 72107838435069186155435662884062257473692284509516, 20849603980134001723930671666823555245252804609722, 53503534226472524250874054075591789781264330331690, ] print(Solution_13(nums)) After this code was run, the terminal would show the following results, and the final answer is 5537376230.\nyuanfeng@yuans-MacBook-Air 823 % python Euler_13.py 5537376230  Problem 31 Coin sums - Solved by 83654  In this question, we are asked:\nIn the United Kingdom the currency is made up of pound (£) and pence (p). There are eight coins in general circulation:\n1p, 2p, 5p, 10p, 20p, 50p, £1 (100p), and £2 (200p). It is possible to make £2 in the following way:\n1×£1 + 1×50p + 2×20p + 1×5p + 1×2p + 3×1p How many different ways can £2 be made using any number of coins? link here\nThis is a typical problem of Dynamic Programming. Based on the descriptions, the order of the coins does not matter, and there will be enough coins to make each combination. I followed the normal way of solving DP problems by going from a small base case, and dive deeper into more complex cases. Tablation is an optimal choice to reduce the space complexity, I built a table with all the possible combinations given the total amount value.\nHere is a table of base case: only the 1 penny is used as indicated in the second column, and as the target increase, there is only one way to reach the targeted amount.\n   target only 1 p     0p 1   1p 1   2p 1   3p 1   4p 1    We then move forward to using both 1 penny and 2 penny coins. The following table shows the ways of coin combinations to reach the targeted amount. Since 0 and 1 are number below 2, only 1 penny is used. As we move to higher value of targets, if the difference between two target values is the same as a given coin, then we have one more possible combination by simply adding one more coin.\n   target 1p, 2p     0p 1   1p 1   2p 2   3p 2   4p 3    To generalize the previous process, we ran over all the possible coins. For each coin, we modify the table from values in the target column that equals the value of the coin until we reach the final target value. This is a quite straightforward algorithm that could be implemented by the following code in Python:\ndef Solution_31(target): \u0026#34;\u0026#34;\u0026#34; Dynamic programming to count the number of coin combinations \u0026#34;\u0026#34;\u0026#34; #convert the amount to pennies sum = target*100 #total values in the target column ttl = [1] + [0] * sum #list of coins to choose from coins = [1, 2, 5, 10, 20, 50, 100, 200] #as we move towards to coins of larger value, update the table by adding new possible combinations for coin in coins: for i in range(sum + 1 - coin): ttl[i + coin] += ttl[i] #return the result return str(ttl[-1]) print(Solution_31(2)) # result: 73682  Problem 31 Coin sums - Solved by 23296  In this question, we are asked:\nFind the unique positive integer whose square has the form 1_2_3_4_5_6_7_8_9_0, where each “_” is a single digit.\nFilling in the blank spaces with 0 or 9 will give us the minimum or maximum value of the square. We take the square root and the range of answer is [1010101010, 1389026623].\nThe last digit in the required form is 0, therefore the number we hope to find should also end in 0, this in turn would mean its square will end in two 0s. This will further reduce the range of answers to [101010101, 138902662]. Now we could look at the second-last number. Since its square ends in 900, the target number should end in either 70 or 30.\nNow we have reduced the search range to a much smaller scale. We first generate the minimum and maximum values, then write a for loop to go through each number in this range. If the last two numbers are 70 or 30, we then check if its square fits in 1_2_3_4_5_6_7_8_9_0.\nHere is an implementation of the previous algorithm.\nimport math def helper(n): \u0026#34;\u0026#34;\u0026#34; determines if the given number fits in 1_2_3_4_5_6_7_8_9_0 \u0026#34;\u0026#34;\u0026#34; for i in range(8,0,-1): n = int(n / 100); if n % 10 != i: return False; return True; def Solution_206(): \u0026#34;\u0026#34;\u0026#34; generate the range of answers and check each value \u0026#34;\u0026#34;\u0026#34; min_val = int(math.sqrt(10203040506070809) / 10); max_val = int(math.sqrt(19293949596979899) / 10) + 1; for i in range(min_val, max_val): #if the value ends in 3 or 7, we check its square num = i * 10 + 3; #if accepted, break the loop and print the value if helper(num*num): break; num = i * 10 + 7; if helper(num* num): break; print(num * 10); #result: 1389019170 Congratulations! Now you have figured out three interesting questions from Project Euler. If you find this article interesting, feel free to check out Project Euler\nScreenshots of Solved problems\n","date":1615161600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1615161600,"objectID":"bef2f59aa29324b051aba2839bc2a1b3","permalink":"https://yuan-feng1.github.io/project/eu/","publishdate":"2021-03-08T00:00:00Z","relpermalink":"/project/eu/","section":"project","summary":"Problem 13 Large sum - Solved by 224972  In this question, we are asked to:\nWork out the first ten digits of the sum of the following one-hundred 50-digit numbers. link here\nThe thought process of this problem is quite straightforward: we first sum up the one-hundred 50-digit numbers, then with the result, turn it into a string, then slicing the first ten digits would solve the problem.","tags":["Demo"],"title":"Project Euler Solutions","type":"project"},{"authors":["Yuan Feng"],"categories":[],"content":"from IPython.core.display import Image Image(\u0026#39;https://www.python.org/static/community_logos/python-logo-master-v3-TM-flattened.png\u0026#39;) print(\u0026#34;Welcome to academia!\u0026#34;) Welcome to academia!  Install Python and Jupyter Install Anaconda which includes Python 3 and Jupyter notebook.\nOtherwise, for advanced users, install Jupyter notebook with pip3 install jupyter.\nCreate a new blog post as usual Run the following commands in your Terminal, substituting \u0026lt;MY_WEBSITE_FOLDER\u0026gt; and my-post with the file path to your academia website folder and a name for your blog post (without spaces), respectively:\ncd \u0026lt;MY_WEBSITE_FOLDER\u0026gt; hugo new --kind post post/my-post cd \u0026lt;MY_WEBSITE_FOLDER\u0026gt;/content/post/my-post/ Create or upload a Jupyter notebook Run the following command to start Jupyter within your new blog post folder. Then create a new Jupyter notebook (New \u0026gt; Python Notebook) or upload a notebook.\njupyter notebook Convert notebook to Markdown jupyter nbconvert Untitled.ipynb --to markdown --NbConvertApp.output_files_dir=. # Copy the contents of Untitled.md and append it to index.md: cat Untitled.md | tee -a index.md # Remove the temporary file: rm Untitled.md Edit your post metadata Open index.md in your text editor and edit the title etc. in the front matter according to your preference.\nTo set a featured image, place an image named featured into your post\u0026rsquo;s folder.\nFor other tips, such as using math, see the guide on writing content with academia.\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"6e929dc84ed3ef80467b02e64cd2ed64","permalink":"https://yuan-feng1.github.io/post/jupyter/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/post/jupyter/","section":"post","summary":"Learn how to blog in academia using Jupyter notebooks","tags":[],"title":"Display Jupyter Notebooks with academia","type":"post"},{"authors":[],"categories":[],"content":"Welcome to Slides academia\n Features  Efficiently write slides in Markdown 3-in-1: Create, Present, and Publish your slides Supports speaker notes Mobile friendly slides   Controls  Next: Right Arrow or Space Previous: Left Arrow Start: Home Finish: End Overview: Esc Speaker notes: S Fullscreen: F Zoom: Alt + Click PDF Export: E   Code Highlighting Inline code: variable\nCode block:\nporridge = \u0026#34;blueberry\u0026#34; if porridge == \u0026#34;blueberry\u0026#34;: print(\u0026#34;Eating...\u0026#34;)  Math In-line math: $x + y = z$\nBlock math:\n$$ f\\left( x \\right) = ;\\frac{{2\\left( {x + 4} \\right)\\left( {x - 4} \\right)}}{{\\left( {x + 4} \\right)\\left( {x + 1} \\right)}} $$\n Fragments Make content appear incrementally\n{{% fragment %}} One {{% /fragment %}} {{% fragment %}} **Two** {{% /fragment %}} {{% fragment %}} Three {{% /fragment %}} Press Space to play!\nOne Two Three  A fragment can accept two optional parameters:\n class: use a custom style (requires definition in custom CSS) weight: sets the order in which a fragment appears   Speaker Notes Add speaker notes to your presentation\n{{% speaker_note %}} - Only the speaker can read these notes - Press `S` key to view {{% /speaker_note %}} Press the S key to view the speaker notes!\n Only the speaker can read these notes Press S key to view    Themes  black: Black background, white text, blue links (default) white: White background, black text, blue links league: Gray background, white text, blue links beige: Beige background, dark text, brown links sky: Blue background, thin dark text, blue links    night: Black background, thick white text, orange links serif: Cappuccino background, gray text, brown links simple: White background, black text, blue links solarized: Cream-colored background, dark green text, blue links   Custom Slide Customize the slide style and background\n{{\u0026lt; slide background-image=\u0026#34;/img/boards.jpg\u0026#34; \u0026gt;}} {{\u0026lt; slide background-color=\u0026#34;#0000FF\u0026#34; \u0026gt;}} {{\u0026lt; slide class=\u0026#34;my-style\u0026#34; \u0026gt;}}  Custom CSS Example Let\u0026rsquo;s make headers navy colored.\nCreate assets/css/reveal_custom.css with:\n.reveal section h1, .reveal section h2, .reveal section h3 { color: navy; }  Questions? Ask\nDocumentation\n","date":1549324800,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1549324800,"objectID":"0e6de1a61aa83269ff13324f3167c1a9","permalink":"https://yuan-feng1.github.io/slides/example/","publishdate":"2019-02-05T00:00:00Z","relpermalink":"/slides/example/","section":"slides","summary":"An introduction to using academia's Slides feature.","tags":[],"title":"Slides","type":"slides"},{"authors":["Yuan Feng"],"categories":[],"content":"Create a free website with academia using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n Setup academia Get Started View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of academia:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an academia sticker Wear the T-shirt    \nKey features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Color Themes academia comes with day (light) and night (dark) mode built-in. Click the sun/moon icon in the top right of the Demo to see it in action!\nChoose a stunning color and font theme for your site. Themes are fully customizable and include:\n         Ecosystem  academia Admin: An admin tool to import publications from BibTeX or import assets for an offline site academia Scripts: Scripts to help migrate content to new versions of academia  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"737a4d9f40e4da19ee82ee5cb6985167","permalink":"https://yuan-feng1.github.io/post/snail/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/snail/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["academia"],"title":"A Study of Snail Behavior","type":"post"},{"authors":["Yuan Feng"],"categories":[],"content":"Create a free website with academia using Markdown, Jupyter, or RStudio. Choose a beautiful color theme and build anything with the Page Builder - over 40 widgets, themes, and language packs included!\nCheck out the latest demo of what you\u0026rsquo;ll get in less than 10 minutes, or view the showcase of personal, project, and business sites.\n Setup academia Get Started View the documentation Ask a question Request a feature or report a bug Updating? View the Update Guide and Release Notes Support development of academia:  Donate a coffee Become a backer on Patreon Decorate your laptop or journal with an academia sticker Wear the T-shirt    \nKey features:\n Page builder - Create anything with widgets and elements Edit any type of content - Blog posts, publications, talks, slides, projects, and more! Create content in Markdown, Jupyter, or RStudio Plugin System - Fully customizable color and font themes Display Code and Math - Code highlighting and LaTeX math supported Integrations - Google Analytics, Disqus commenting, Maps, Contact Forms, and more! Beautiful Site - Simple and refreshing one page design Industry-Leading SEO - Help get your website found on search engines and social media Media Galleries - Display your images and videos with captions in a customizable gallery Mobile Friendly - Look amazing on every screen with a mobile friendly version of your site Multi-language - 15+ language packs including English, 中文, and Português Multi-user - Each author gets their own profile page Privacy Pack - Assists with GDPR Stand Out - Bring your site to life with animation, parallax backgrounds, and scroll effects One-Click Deployment - No servers. No databases. Only files.  Color Themes academia comes with day (light) and night (dark) mode built-in. Click the sun/moon icon in the top right of the Demo to see it in action!\nChoose a stunning color and font theme for your site. Themes are fully customizable and include:\n         Ecosystem  academia Admin: An admin tool to import publications from BibTeX or import assets for an offline site academia Scripts: Scripts to help migrate content to new versions of academia  Install You can choose from one of the following four methods to install:\n one-click install using your web browser (recommended) install on your computer using Git with the Command Prompt/Terminal app install on your computer by downloading the ZIP files install on your computer with RStudio  Then personalize and deploy your new site.\nUpdating View the Update Guide.\nFeel free to star the project on Github to help keep track of updates.\nLicense Copyright 2016-present George Cushen.\nReleased under the MIT license.\n","date":1461110400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1555459200,"objectID":"279b9966ca9cf3121ce924dca452bb1c","permalink":"https://yuan-feng1.github.io/post/getting-started/","publishdate":"2016-04-20T00:00:00Z","relpermalink":"/post/getting-started/","section":"post","summary":"Create a beautifully simple website in under 10 minutes.","tags":["academia"],"title":"academia: the website builder for Hugo","type":"post"}]